# 3 基于条件编码长短期记忆社交媒体文本立场分析
## 引言
当前社交媒体文本立场分析的研究中，研究人员的研究方向主要集中在如何提取社交媒体文本有效的分类特征。忽略了文本立场分析一种重要的出发点是文本基于某个特定的目标，若原文本脱离了特定目标，文本立场分析与情感分析将无差别。基于这个出发点，本章提出了基于条件编码长短期记忆神经网络的模型，通过以条件编码的形式引入文本的目标消息，使立场分析的效果得到显著的提升。本章研究基于条件编码长短期记忆的社交媒体文本立场分析方法，通过以不同形式给模型接入文本立场的目标消息，表明了接入目标信息对文本立场分析有明显提升效果。通过在SemEval2016英文立场分析数据集和NLPCC2016中文立场分析数据集的实验，论证上述结论。<br>
本章的各节结构如下:3.2节介绍条件编码长短期记忆神经网络模型;3.3节介绍基条件编码长短期记忆在文本立场分析;3.4 节为本章实验和结果分析;最后一节为本章小结。
## 条件编码长短期记忆神经网络模型
循环神经网络(Recurrent Neural Networks-RNN)已经在众多自然语言处理任务上取得了巨大成功。不同于传统过得前向反馈神经网络的同层节点无连接层于层之间节点有连接，循环神经网络引入了定向循环，可以处理序列数据。RNN中最大的缺陷是后面时间的节点对于前面节点的感知力下降，当网络深时训练效果下降。LSTM可以解决这一问题，目前LSTM是RNN中使用最广泛最成功的模型。Rocktaschel[???]在2016年在句子之间的文本蕴含识别（Recognizing textual entailment RTE）的研究中提出了条件编码的思想，其论证了在文本含义识别任务上，条件编码比单独编码更能抽取两个句子之间的信息。结合文本立场分析的也有文本和目标需要同时考虑特点，把借鉴条件编码的思想来解决文本立场分析的任务。<br>
### 基于GloVe的词嵌入模型
词的表示是自然语音处理中一个基础且十分重要的任务，大量的研究人员投入到词表示的研究中。早期的有词的表示方式为one hot encoding。每个词独占一个维度，每个词向量有一个维度是1，其他维度为0，词向量的维度是所以单词的的长度。One hot编码的特点是假设所有的单词互相独立，这是一个很强的假设，显然在有些任务中并不合适，如词语相似度方面，dog和cat的相似度应当比dog和not高，但是在one hot编码中他们相似性一样。
### 长短期记忆神经网络模型
由于文本序列的通常具有较长的长度，导致神经网络的层数较多，而传统的递归神经网络解决序列问题经常会出现梯度消失的问题(vanishing gradient problem)与梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。出现的原因在于对神经网络参数进行链式求导的过程中，输出对于前面递归神经参数的倒数随着累乘激活函数的导数而接近于0，以下图的反向传播为例（假设每一层只有一个神经元且对于每一层$y_i=\sigma(z_i)=\sigma(w_ix_i+b_i)$，其中$\sigma$为sigmoid函数）
# rnn.png #
可以推导出
$$\frac{\partial C}{\partial b_1} = \frac{\partial C}{\partial y_4}\frac{\partial y_4}{\partial z_4}\frac{\partial z_4}{\partial x_4}\frac{\partial x_4}{\partial z_3}\frac{\partial z_3}{\partial x_3}\frac{\partial x_3}{\partial z_2}\frac{\partial z_2}{\partial x_2}\frac{\partial x_2}{\partial z_1}\frac{\partial z_1}{\partial b_1}$$
$$=\frac{\partial C}{\partial y_4}\ \sigma ^\prime(z_4)w_4\sigma ^\prime(z_3)w_3\sigma ^\prime(z_2)w_2\sigma ^\prime(z_1)$$
一般的非线性激活函数的导数都小于1（例如sigmoid的导数最大值为$\frac{1}{4}$）,因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，因而导致梯度消失的情况出现。<br>
长短期记忆（Long Short-Term Memory, LSTM）是一种缓解上述问题的间递归神经网络的变种，Hochreiter在1997年首次提出了LSTM结构，2000年Gers等人改进LSTM模型。LSTM单元结构如下
# LSTM.png#
LSTM模型提出了记忆存储格（memory cell）的结构，内部包含了遗忘门（forget gate）/？输入门（input gate） /？输出门（output gata） 。各种门的作用在于调节记忆体在外部输入的情况下应该采取怎么的存储测量，具体门状态和记忆体内部参数的更新公式如下。
$$i_t=\sigma_g(W^ix_t+U_ih_{t-1}+b^i)$$
$$f_t=\sigma_g(W^fx_t+U_fh_{t-1}+b^f)$$
$$o_t=\sigma_g(W^ox_t+U_oh_{t-1}+b^o)$$
$$c_t=f_t \odot c_{t-1}+i_t\odot \sigma_c(W_cx_t+U_ch_{t-1}+b_c)$$
$$h_t=o_t \odot \sigma_h(c_t)$$
其中$\sigma_g$为sigmoid的激活函数，$\sigma_c, \sigma_h$为thah的激活函数，$x_t$为输入向量，$h_t$为输出向量， $c_t$为记忆向量，$W,U和b$是矩阵参数和向量参数。各个门的值是保持在0-1之间的向量。其中遗忘门向量$f_t$表示上一时刻的记忆体信息需要遗忘多少， 输入门向量$i_t$表示有多少当前时刻输入信息需要加入到记忆体中，输出门向量$o_t$表示记忆体输出多少信息。<br>
前已经证明，LSTM是解决长序依赖问题的有效技术，并且这种技术的普适性非常高，导致带来的可能性变化非常多。各研究者根据LSTM纷纷提出了自己的变量版本，这就让LSTM可以处理千变万化的垂直问题。
### 条件编码长短期记忆神经网络模型
Rocktaschel[]等在句子之间的文本蕴含识别的研究中提出了条件编码长短期记忆神经网络模型，文本蕴含定义为一对文本之间的有向推理关系，其中蕴含前件记作T(Text)，蕴含后件记作H(Hypothesis)。如果人们依据自己的常识认为H的语义能够由T的语义推理得出的话，那么称T蕴含H，记作T → H, 作者提出的模型的结构是首先有一个LSTM模型编码Text消息，另一个不同参数的LSTM模型编码Hypothesis。作者不是简单把两个特征向量拼接在一起，而是做了如下转换。把第一个编码Text信息的LSTM模型的记忆状态（Cell）保留下来，作为第二个编码Hypothesis的LSTM模型记忆状态（Cell）的初始值，此模型建立的了Text消息作为条件下的对Hypothesis的编码表示。
# conditional_encoding.png #
如上图图表示所示“A wedding party taking pictures”作为我们的Text文本，“Someone got married”作为我们的Hypothesis，其中$c_5$作为前一个LSTM的记忆体状态被当做编码Hypothesis的初始记忆状态。两个LSTM具体的状态转移公式如下：
$$[h_1~c_1] = LSTM^{Text}(x_1,h_0,c_0)$$
$$...$$
$$[h_T~c_T] = LSTM^{Text}(x_1,h_{T-1},c_{T-1})$$
$$[h_{T+1}~c_{T+1}] = LSTM^{Hypothesis}(x_1,h_0,c_T)$$
$$...$$
$$[h_{N}~c_{N}] = LSTM^{Hypothesis}(x_1,h_{N-1},c_{N-1})$$
$$c=tanh(Wh_N)$$
其中$(x_1...x_T)$为Text的序列消息，$(x_{T+1}...x_N)$为Hypothesis的序列信息。$h_0,c_0$为LSTM的初始化向量。<br>
实验证明在文本蕴含任务上，条件LSTM模型比单独编码高3.3%（从77.6%提升到80.9%）的性能。这种条件编码能使Text的信息更好的流向对Hypothesis编码的LSTM模型，有了第一个LSTM模型传来的记忆状态，第二个LSTM模型能更好的编码Hypothesis的消息。

## 基于条件编码长短期记忆的文本立场分析
通过Rocktaschel在文本蕴含的任务的实验可知，在处理两个文本序列的编码任务上，条件编码长短期记忆神经网络比单独独立编码两个文本序列有更好的建模能力。在文本立场分析的任务上，有文本的信息和目标主题两个文本序列信息，我们可以借鉴条件编码长短期记忆神经网络在文本蕴含建模的方式，把文本信息和目标主题信息更好结合起来。在实验部分设计了多种文本序列信息的结合方式，通过实验证明了以目标主题文本作为条件编码文本信息的模型对文本立场分析有更好的效果。<br>

本文后期实验将在NLPCC2016中文微博立场分析数据集和SemEval2016英文Twitter立场分析数据集，为较清晰阐述条件编码长短期记忆模型，以下简短的介绍下两个数据集的样例，具体的有关数据集的信息将会在下面实验部分做详细介绍。<br>
NLPCC 2016数据集：<br>
例1:目标主题文本:"深圳禁摩限电" 微博文本:"支持深圳交警。电单车继续治理" 立场分析类标：“Favor”（持支持立场）<br>
目标的文本主题有关“深圳禁摩限电”的主题的，而从微博文本“支持深圳交警。电单车继续治理”中，我们可以知道微博的作者首先是赞同了深圳交警的行为，然后叙述了电单车需要得到继续的整治，从两个方法肯定了"深圳禁摩限电"这个主题目标的，因此给出的类标是“Favor”也就是持支持目标主题的立场。<br>
例1:目标主题文本:"Hillary Clinton" Twitter文本:"Hopefully Hillary Clinton gets cancer and dies before she gets the opportunity to embarrass our country any further“，立场分析类标“Against”（持反对立场）<br>
译文:"真希望希拉里克林顿得癌症然后死去，这样她就不再会有机会再让我们国家蒙羞了。"<br>
目标的文本主题有关“希拉里克林顿”的主题的，这个Twitter文本是有关2016年美国大选，显然Twitter作者一直咒骂希拉里克林顿，希望她得癌症，不让她侮辱国家，可以看出作者有强烈反对主题目标“希拉里克林顿”，因此给出的类标是“Against”，也就是持反对目标主题的立场。<br>
从上述的两个简单的样例可知，立本立场是有两个输入的，一个是立场主题例如“深圳禁摩限电”和“Hillary Clinton”。另外一个是立场下的文本“支持深圳交警。电单车继续治理”和”Hopefully Hillary Clinton gets cancer and dies before she gets the opportunity to embarrass our country any further“。在这通过中文微博阐述条件编码长短期记忆模型的建立。首先经过一些数据预处理和分词把主题目标”深圳禁摩限电“和”支持深圳交警。电单车继续治理”转变成”深圳 禁摩 限电“和”支持 深圳 交警“。

在文本立场分析的任务中
# 深圳禁摩限电 支持深圳交警。电单车继续治理.png#
## 实验设置及结果
## 本章小节

